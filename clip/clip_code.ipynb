{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepSeek提供的原始的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import FlavaModel\n",
    "\n",
    "def capture_flava_activations(model):\n",
    "    # 存储钩子的引用\n",
    "    hooks = []\n",
    "    \n",
    "    # 存储各层的数据\n",
    "    attention_data = {}  # 保存每层的Q和K\n",
    "    ffn_outputs = {}     # 保存每层FFN的输出\n",
    "    ffn_weights = {}     # 保存每层FFN的线性层权重\n",
    "\n",
    "    # 遍历模型的encoder层（以image_model为例）\n",
    "    encoder = model.image_model.encoder\n",
    "    for layer_idx, layer in enumerate(encoder.layers):\n",
    "        # 获取当前层的SelfAttention模块\n",
    "        # self_attention = layer.attention.attention\n",
    "        self_attention = layer.self_attn\n",
    "        \n",
    "        # 注册钩子捕获Q和K\n",
    "        def q_hook(module, input, output, idx=layer_idx):\n",
    "            attention_data.setdefault(idx, {})['Q'] = output.detach()\n",
    "        hook_q = self_attention.query.register_forward_hook(q_hook)\n",
    "        hooks.append(hook_q)\n",
    "        \n",
    "        def k_hook(module, input, output, idx=layer_idx):\n",
    "            attention_data.setdefault(idx, {})['K'] = output.detach()\n",
    "        hook_k = self_attention.key.register_forward_hook(k_hook)\n",
    "        hooks.append(hook_k)\n",
    "        \n",
    "        # 保存FFN的权重（中间层和输出层）\n",
    "        intermediate_weights = layer.intermediate.dense.weight.data.clone()\n",
    "        output_weights = layer.output.dense.weight.data.clone()\n",
    "        ffn_weights[layer_idx] = {\n",
    "            'intermediate': intermediate_weights,\n",
    "            'output': output_weights\n",
    "        }\n",
    "        \n",
    "        # 注册钩子捕获FFN的输出（FlavaOutput的输出）\n",
    "        def ffn_hook(module, input, output, idx=layer_idx):\n",
    "            ffn_outputs[idx] = output.detach()\n",
    "        hook_ffn = layer.output.register_forward_hook(ffn_hook)\n",
    "        hooks.append(hook_ffn)\n",
    "    \n",
    "    return hooks, attention_data, ffn_outputs, ffn_weights\n",
    "\n",
    "# 使用示例\n",
    "model = FlavaModel.from_pretrained(\"facebook/flava-full\")  # 加载模型\n",
    "\n",
    "# 注册钩子\n",
    "hooks, attention_data, ffn_outputs, ffn_weights = capture_flava_activations(model)\n",
    "\n",
    "# 准备输入数据（示例）\n",
    "inputs = {\n",
    "    \"pixel_values\": torch.randn(1, 3, 224, 224),  # 示例图像输入\n",
    "    \"input_ids\": torch.randint(0, 30522, (1, 77)), # 示例文本输入\n",
    "}\n",
    "\n",
    "# 前向传播，触发钩子\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 移除钩子\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# 打印结果示例\n",
    "print(\"Q values for layer 0:\", attention_data[0]['Q'].shape)\n",
    "print(\"FFN output for layer 0:\", ffn_outputs[0].shape)\n",
    "print(\"FFN intermediate weights shape:\", ffn_weights[0]['intermediate'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP-Text (Q, K, FFN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出结果维度（层数，1，样本数，tokens数，hidden_size）\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "\n",
    "# 0. ==========定义钩子==========\n",
    "def capture_flava_activations(model):\n",
    "    # 存储钩子的引用\n",
    "    hooks = []\n",
    "\n",
    "    # 遍历模型的encoder层（以image_model为例）\n",
    "    encoder = model.text_model.encoder\n",
    "    \n",
    "    # 存储各层的数据\n",
    "    q_list = [[] for _ in range(len(encoder.layers))]  # 保存每层的Q和K\n",
    "    k_list = [[] for _ in range(len(encoder.layer))]  # 保存每层的Q和K\n",
    "    ffn_outputs = [[] for _ in range(len(encoder.layer))]    # 保存每层FFN的输出\n",
    "    ffn_weights = {}     # 保存每层FFN的线性层权重\n",
    "\n",
    "\n",
    "    for layer_idx, layer in enumerate(encoder.layer):\n",
    "        # 获取当前层的SelfAttention模块\n",
    "        self_attention = layer.attention.attention\n",
    "        \n",
    "        # 注册钩子捕获Q和K\n",
    "        def q_hook(module, input, output, idx=layer_idx):\n",
    "            q_list[idx].append(output.detach().cpu().numpy())\n",
    "        hook_q = self_attention.query.register_forward_hook(q_hook)\n",
    "        hooks.append(hook_q)\n",
    "        \n",
    "        def k_hook(module, input, output, idx=layer_idx):\n",
    "            k_list[idx] = output.detach()\n",
    "        hook_k = self_attention.key.register_forward_hook(k_hook)\n",
    "        hooks.append(hook_k)\n",
    "        \n",
    "        # 保存FFN的权重（中间层和输出层）\n",
    "        intermediate_weights = layer.intermediate.dense.weight.data.clone()\n",
    "        output_weights = layer.output.dense.weight.data.clone()\n",
    "        ffn_weights[layer_idx] = {\n",
    "            'intermediate': intermediate_weights,\n",
    "            'output': output_weights\n",
    "        }\n",
    "        \n",
    "        # 注册钩子捕获FFN的输出（FlavaOutput的输出）\n",
    "        def ffn_hook(module, input, output, idx=layer_idx):\n",
    "            ffn_outputs[idx] = output.detach().cpu().numpy()\n",
    "        hook_ffn = layer.output.register_forward_hook(ffn_hook)\n",
    "        hooks.append(hook_ffn)\n",
    "    \n",
    "    return hooks, q_list, k_list, ffn_outputs, ffn_weights\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"D:/dev/code/HuggingFace/pretrainedModel/clip-vit-base-patch32\" # 模型名称\n",
    "\n",
    "    # 设置设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1. ==========加载模型和分词器==========\n",
    "    model = CLIPModel.from_pretrained(model_name)  # 加载模型\n",
    "    tokenizer = CLIPProcessor.from_pretrained(model_name)\n",
    "    model.text_model.encoder.layers.self_attn\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    print(f\"模型已加载至 {device}\")\n",
    "\n",
    "\n",
    "    # 2. ==========准备输入文本==========\n",
    "    texts = [\n",
    "        \"一只函数的返回大幅改进企鹅瑞华企鹅舞i意见猫\", \n",
    "        \"一只猫和一啊但是发射点发射点只狗\", \n",
    "        \"as阿凡达发hpoerujhiopertfasfa\", \n",
    "        \"放噶撒旦发射覅殴打事件回顾i哦速度返回结果点\"\n",
    "    ]\n",
    "    inputs = tokenizer(\n",
    "        text=texts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    # 3. ==========注册钩子==========\n",
    "    hooks, q_list, k_list, ffn_outputs, ffn_weights = capture_flava_activations(model)\n",
    "\n",
    "\n",
    "    # 4. ==========前向传播，触发钩子==========\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "\n",
    "    # 5. ==========移除钩子==========\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # # 打印结果示例\n",
    "    # print(\"Q values for layer 0:\", attention_data[0]['Q'].shape)\n",
    "    # print(\"FFN output for layer 0:\", ffn_outputs[0].shape)\n",
    "    # print(\"FFN intermediate weights shape:\", ffn_weights[0]['intermediate'].shape)\n",
    "\n",
    "    # # 6. ==========保存所有数据为NumPy数组==========\n",
    "    # output_dir = \"./flava_full_outputs\"\n",
    "    # import os\n",
    "    # if not os.path.exists(output_dir):\n",
    "    #     os.makedirs(output_dir)\n",
    "\n",
    "    # # 保存Q、K和FFN输出\n",
    "    # for i in range(len(model.text_model.encoder.layer)):\n",
    "    #     # 保存Q\n",
    "    #     np.save(f'{output_dir}/q_layer_{i+1}.npy', np.array(q_list[i]))\n",
    "\n",
    "    # ffn_outputs_np = np.array(ffn_outputs)\n",
    "    # print(ffn_outputs_np.shape)\n",
    "\n",
    "\n",
    "    # import gc   # 清理内存\n",
    "    # gc.collect()    # 清理内存\n",
    "    # del q_list, k_list, ffn_outputs, ffn_weights\n",
    "    # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepSeek又一版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "from transformers import FlavaModel, FlavaProcessor\n",
    "\n",
    "# 存储权重和激活的字典\n",
    "activations = defaultdict(dict)\n",
    "weights = defaultdict(dict)\n",
    "\n",
    "def get_hooks(layer_idx):\n",
    "    \"\"\"为指定层定义前向钩子\"\"\"\n",
    "    \n",
    "    # 捕获 SelfAttention 的 key 和 query\n",
    "    def attention_hook(module, input, output):\n",
    "        # output 是 (context_layer,) 或 (context_layer, attention_probs)\n",
    "        with torch.no_grad():\n",
    "            query = module.query(input[0])\n",
    "            key = module.key(input[0])\n",
    "            \n",
    "            # 转置为 (batch, heads, seq_len, dim_per_head)\n",
    "            query_layer = module.transpose_for_scores(query)\n",
    "            key_layer = module.transpose_for_scores(key)\n",
    "            \n",
    "        activations[layer_idx]['query'] = query_layer.detach()\n",
    "        activations[layer_idx]['key'] = key_layer.detach()\n",
    "\n",
    "    # 捕获 FFN 中间层权重和输出\n",
    "    def ffn_intermediate_hook(module, input, output):\n",
    "        # 中间层的 dense 权重\n",
    "        weights[layer_idx]['ffn_intermediate_weight'] = module.dense.weight.detach().clone()\n",
    "        # 中间层的输出激活\n",
    "        activations[layer_idx]['ffn_intermediate_output'] = output.detach()\n",
    "\n",
    "    # 捕获 FFN 输出层权重和输出\n",
    "    def ffn_output_hook(module, input, output):\n",
    "        # 输出层的 dense 权重\n",
    "        weights[layer_idx]['ffn_output_weight'] = module.dense.weight.detach().clone()\n",
    "        # 输出层的输出激活\n",
    "        activations[layer_idx]['ffn_output_output'] = output.detach()\n",
    "\n",
    "    return attention_hook, ffn_intermediate_hook, ffn_output_hook\n",
    "\n",
    "def register_hooks(model):\n",
    "    \"\"\"遍历模型并为每一层注册钩子\"\"\"\n",
    "    \n",
    "    # 假设处理图像编码器\n",
    "    encoder = model.image_model.encoder\n",
    "    \n",
    "    for layer_idx, layer in enumerate(encoder.layer):\n",
    "        # 注册 SelfAttention 钩子\n",
    "        attention = layer.attention.attention\n",
    "        attn_hook = attention.register_forward_hook(get_hooks(layer_idx)[0])\n",
    "        \n",
    "        # 注册 FFN 中间层和输出层钩子\n",
    "        intermediate = layer.intermediate\n",
    "        ffn_inter_hook = intermediate.register_forward_hook(get_hooks(layer_idx)[1])\n",
    "        \n",
    "        output = layer.output\n",
    "        ffn_out_hook = output.register_forward_hook(get_hooks(layer_idx)[2])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 使用示例\n",
    "model_name = \"C:/Users/xinlong/Desktop/code/python/flava_use/model/facebook/flava-full\"\n",
    "\n",
    "model = FlavaModel.from_pretrained(model_name)\n",
    "tokenizer = FlavaProcessor.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"模型已加载至 {device}\")\n",
    "\n",
    "# 注册钩子到图像编码器\n",
    "register_hooks(model)\n",
    "\n",
    "# 假设输入数据\n",
    "pixel_values = torch.randn(1, 3, 224, 224).to(device)  # 示例图像输入\n",
    "input_ids = torch.randint(0, 30522, (1, 77)).to(device) # 示例文本输入\n",
    "\n",
    "# 运行前向传播\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values=pixel_values, input_ids=input_ids)\n",
    "\n",
    "# 查看捕获的权重\n",
    "for layer_idx in weights.keys():\n",
    "    print(f\"Layer {layer_idx} FFN Intermediate Weight Shape:\", weights[layer_idx]['ffn_intermediate_weight'].shape)\n",
    "    print(f\"Layer {layer_idx} FFN Output Weight Shape:\", weights[layer_idx]['ffn_output_weight'].shape)\n",
    "print(weights[0]['ffn_intermediate_weight'])\n",
    "print(weights[0]['ffn_output_weight'])\n",
    "# 示例输出：\n",
    "# Layer 0 FFN Intermediate Weight Shape: torch.Size([3072, 768])  # 假设 hidden_size=768, intermediate_size=3072\n",
    "# Layer 0 FFN Output Weight Shape: torch.Size([768, 3072])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPTokenizer\n",
    "\n",
    "\n",
    "# 存储文本和视觉编码器的中间结果\n",
    "text_attention_outputs = []  # 各层自注意力的key和query\n",
    "vision_attention_outputs = []\n",
    "text_ffn_outputs = []        # 各层FFN的输出\n",
    "vision_ffn_outputs = []\n",
    "text_ffn_weights = []        # 各层FFN的权重\n",
    "vision_ffn_weights = []\n",
    "hooks = []                    # 用于保存钩子以便移除\n",
    "\n",
    "# 定义钩子函数\n",
    "def register_hooks(encoder, is_text=True):\n",
    "    encoder_hooks = []\n",
    "    for layer_idx, layer in enumerate(encoder.layers):\n",
    "        # 注册 SelfAttention 钩子\n",
    "        attn = layer.self_attn\n",
    "        # 设置父模块以获取参数\n",
    "        attn.k_proj.parent = attn\n",
    "        attn.q_proj.parent = attn\n",
    "\n",
    "        # 注册 k_proj的钩子\n",
    "        def hook_k_proj(module, input, output, layer_idx=layer_idx, is_text=is_text):\n",
    "            parent = module.parent\n",
    "            bsz, seq_len, _ = output.shape\n",
    "            num_heads = parent.num_heads\n",
    "            head_dim = parent.head_dim\n",
    "            key_states = output.view(bsz, seq_len, num_heads, head_dim).transpose(1, 2).contiguous()\n",
    "            target = text_attention_outputs if is_text else vision_attention_outputs\n",
    "            while len(target) <= layer_idx:\n",
    "                target.append({'key_layer': [], 'query_layer': []})\n",
    "            target[layer_idx]['key_layer'].append(key_states)\n",
    "        \n",
    "        encoder_hooks.append(attn.k_proj.register_forward_hook(hook_k_proj))\n",
    "\n",
    "        # 注册q_proj的钩子\n",
    "        def hook_q_proj(module, input, output, layer_idx=layer_idx, is_text=is_text):\n",
    "            parent = module.parent\n",
    "            bsz, seq_len, _ = output.shape\n",
    "            num_heads = parent.num_heads\n",
    "            head_dim = parent.head_dim\n",
    "            query_states = output.view(bsz, seq_len, num_heads, head_dim).transpose(1, 2).contiguous()\n",
    "            target = text_attention_outputs if is_text else vision_attention_outputs\n",
    "            while len(target) <= layer_idx:\n",
    "                target.append({'key_layer': [], 'query_layer': []})\n",
    "            target[layer_idx]['query_layer'].append(query_states)\n",
    "        \n",
    "        encoder_hooks.append(attn.q_proj.register_forward_hook(hook_q_proj))\n",
    "\n",
    "        # 注册 FFN 中间层和输出层钩子\n",
    "        mlp = layer.mlp\n",
    "        \n",
    "        def hook_mlp(module, input, output, layer_idx=layer_idx, is_text=is_text):\n",
    "            target_outputs = text_ffn_outputs if is_text else vision_ffn_outputs\n",
    "            target_weights = text_ffn_weights if is_text else vision_ffn_weights\n",
    "            target_outputs.append(output)\n",
    "            target_weights.append({\n",
    "                'fc1': module.fc1.weight.detach().clone(),\n",
    "                'fc2': module.fc2.weight.detach().clone(),\n",
    "            })\n",
    "        \n",
    "        encoder_hooks.append(mlp.register_forward_hook(hook_mlp))\n",
    "    return encoder_hooks\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 模型名称\n",
    "    model_name = \"C:\\\\Users\\\\xinlong\\\\Desktop\\\\code\\\\python\\\\HuggingFace\\\\pretrainedModel\\\\clip-vit-base-patch32\"\n",
    "    # 加载模型\n",
    "    model = CLIPModel.from_pretrained(model_name)\n",
    "    # 分词器\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "    # 模型加载至设备\n",
    "    model.to(device)\n",
    "    print(f\"模型已加载至 {device}\")\n",
    "\n",
    "\n",
    "    # 注册文本编码器钩子\n",
    "    text_encoder = model.text_model.encoder\n",
    "    hooks += register_hooks(text_encoder, is_text=True)\n",
    "\n",
    "    # 注册视觉编码器钩子\n",
    "    vision_encoder = model.vision_model.encoder\n",
    "    hooks += register_hooks(vision_encoder, is_text=False)\n",
    "\n",
    "\n",
    "    # 准备输入数据\n",
    "    # 文本输入\n",
    "    text_inputs = tokenizer(\n",
    "        [\"a photo of a cat\", \"a photo of a dog\", \"asdaasdasggg\", \"的噶啥分割后i的后果偶i\"], \n",
    "        padding=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    # 图像输入（示例）\n",
    "    from PIL import Image\n",
    "    import requests\n",
    "    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    # 清空之前的存储\n",
    "    text_attention_outputs.clear()\n",
    "    vision_attention_outputs.clear()\n",
    "    text_ffn_outputs.clear()\n",
    "    vision_ffn_outputs.clear()\n",
    "    text_ffn_weights.clear()\n",
    "    vision_ffn_weights.clear()\n",
    "\n",
    "\n",
    "    # 前向传播并捕获数据\n",
    "    # outputs = model(input_ids=text_inputs.input_ids, pixel_values=pixel_values)\n",
    "    outputs = model(\n",
    "        input_ids=text_inputs.input_ids,\n",
    "        pixel_values=pixel_values\n",
    "    )\n",
    "\n",
    "    # 移除所有钩子\n",
    "    for hook in hooks:\n",
    "        hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例：访问文本编码器第一层的key和query\n",
    "text_layer0_key = text_attention_outputs[0]['key_layer'][0]\n",
    "text_layer0_query = text_attention_outputs[0]['query_layer'][0]\n",
    "\n",
    "# 示例：访问文本编码器第一层的key和query\n",
    "vision_layer0_key = vision_attention_outputs[0]['key_layer'][0]\n",
    "vision_layer0_query = vision_attention_outputs[0]['query_layer'][0]\n",
    "\n",
    "# 示例：访问视觉编码器第一层的FFN输出和权重\n",
    "vision_ffn_output = vision_ffn_outputs[0]\n",
    "vision_ffn_fc1_weight = vision_ffn_weights[0]['fc1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.vision_model_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_ffn_12_output = vision_ffn_outputs[11]\n",
    "vision_ffn_12_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用hook获取FFN输出并实现交叉注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPTokenizer\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "\n",
    "# 全局变量，用于存储ffn输出\n",
    "text_ffn_outputs = None\n",
    "vision_ffn_outputs = None\n",
    "\n",
    "# 定义钩子函数\n",
    "def text_ffn_hook(module, input, output):\n",
    "    global text_ffn_outputs\n",
    "    text_ffn_outputs = output\n",
    "\n",
    "def vision_ffn_hook(module, input, output):\n",
    "    global vision_ffn_outputs\n",
    "    vision_ffn_outputs = output\n",
    "\n",
    "# 定义交叉注意里模块\n",
    "class CrossAttention(nn.module):\n",
    "    def __init__(self, query_dim, key_dim, value_dim, output_dim, num_head):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.head_dim = output_dim // num_head\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(query_dim, output_dim)\n",
    "        self.k_proj = nn.Linear(key_dim, output_dim)\n",
    "        self.v_proj = nn.Linear(value_dim, output_dim)\n",
    "        self.out_proj = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "\n",
    "        q = q.view(batch_size, -1, self.num_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, -1, self.num_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, -1, self.num_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正在完善的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPPreTrainedModel, CLIPModel, CLIPProcessor, CLIPTokenizer, CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n",
    "from PIL import Image\n",
    "from typing import Optional, Tuple, Union, Any\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "class CLIPWithProjectedCrossAttention(CLIPPreTrainedModel):\n",
    "    def __init__(self, config: CLIPConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # 加载CLIP模型并冻结参数\n",
    "        self.clip = CLIPModel(config)\n",
    "        for param in self.clip.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # 获取编码器的维度信息\n",
    "        text_hidden_size = config.text_config.hidden_size\n",
    "        vision_hidden_size = config.vision_config.hidden_size\n",
    "        common_dim = 768    # 统一投影维度\n",
    "\n",
    "        # # 维度对齐投影层（使用线性层）\n",
    "        # self.text_proj = nn.Linear(text_hidden_size, common_dim)\n",
    "        # self.vision_proj = nn.Linear(vision_hidden_size, common_dim)\n",
    "\n",
    "        # 增强的投影层（使用MLP）\n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(text_hidden_size, common_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(common_dim, common_dim)\n",
    "        )\n",
    "        self.vision_proj = nn.Sequential(\n",
    "            nn.Linear(vision_hidden_size, common_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(common_dim, common_dim)\n",
    "        )\n",
    "\n",
    "        # 交叉注意力层\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=common_dim,\n",
    "            num_heads=12, # 根据需要调整\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # # 初始化投影层参数\n",
    "        # nn.init.xavier_uniform_(self.text_proj.weight)\n",
    "        # nn.init.xavier_uniform_(self.vision_proj.weight)\n",
    "        # self.post_init()\n",
    "\n",
    "        # 参数初始化\n",
    "        nn.init.xavier_uniform_(self.text_proj[0].weight)\n",
    "        nn.init.xavier_uniform_(self.text_proj[2].weight)\n",
    "        nn.init.xavier_uniform_(self.vision_proj[0].weight)\n",
    "        nn.init.xavier_uniform_(self.vision_proj[2].weight)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, torch.Tensor]:\n",
    "        \n",
    "        # 获取编码器原始输出\n",
    "        text_outputs = self.clip.text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "        vision_outputs = self.clip.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "        # 获取最后一层隐藏状态（未投影）\n",
    "        text_features = text_outputs.last_hidden_state\n",
    "        vision_features = vision_outputs.last_hidden_state\n",
    "\n",
    "        # 维度对齐投影\n",
    "        projected_text = self.text_proj(text_features)          # (batch, seq_len. common_dim)\n",
    "        projected_vision = self.vision_proj(vision_features)    # (batch, seq_len, common_dim)\n",
    "        print(f\"projected_text.shape: {projected_text.shape}, projected_vision.shape: {projected_vision.shape}\")\n",
    "\n",
    "        # 归一化\n",
    "        projected_text = projected_text / projected_text.norm(dim=-1, keepdim=True)\n",
    "        projected_vision = projected_vision / projected_vision.norm(dim=-1, keepdim=True)\n",
    "        print(f\"projected_text.shape: {projected_text.shape}, projected_vision.shape: {projected_vision.shape}\")\n",
    "\n",
    "        # 交叉注意力计算（文本作为Query，视觉作为Key/Value）\n",
    "        text_to_vision, _ = self.cross_attn(\n",
    "            query=projected_text,\n",
    "            key=projected_vision,\n",
    "            value=projected_vision,\n",
    "            key_padding_mask=(attention_mask == 0) if attention_mask is not None else None\n",
    "        )\n",
    "        vision_to_text, _ = self.cross_attn(\n",
    "            query=projected_vision,\n",
    "            key=projected_text,\n",
    "            value=projected_text,\n",
    "            key_padding_mask=(attention_mask == 0) if attention_mask is not None else None\n",
    "        )\n",
    "\n",
    "        return text_to_vision, vision_to_text\n",
    "\n",
    "# 对比学习损失函数\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cross_entroy = nn.CrossEntropyLoss\n",
    "    \n",
    "    def forward(self, text_emb, vision_emb):\n",
    "        # 计算相似度矩阵\n",
    "        logits_per_text = text_emb @ vision_emb.t() / self.temperature\n",
    "        logits_per_vision = vision_emb @ text_emb.t() / self.temperature\n",
    "        labels = torch.arange(logits_per_text.size(0), device=text_emb.device)\n",
    "        loss_t = self.cross_entroy(logits_per_text, labels)\n",
    "        loss_v = self.cross_entroy(logits_per_vision, labels)\n",
    "        return (loss_t + loss_v) / 2\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化配置（示例使用不同维度）\n",
    "    text_config = CLIPTextConfig(hidden_size=512)\n",
    "    vision_config = CLIPVisionConfig(hidden_size=768)\n",
    "\n",
    "    text_config_dict = text_config.to_dict()\n",
    "    vision_config_dict = vision_config.to_dict()\n",
    "    config = CLIPConfig(\n",
    "        text_config=text_config_dict, \n",
    "        vision_config=vision_config_dict\n",
    "    )\n",
    "    \n",
    "    model = CLIPWithProjectedCrossAttention(config)\n",
    "    \n",
    "\n",
    "    # ====================训练模型====================\n",
    "    # 打印可训练参数\n",
    "    print(f\"{'='*10}Trainable parameters:{'='*10}\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{'-'*10}{name}\")\n",
    "\n",
    "    # 仅选择需要训练的参数\n",
    "    trainable_params = [\n",
    "        {'params': model.text_proj.parameters()},\n",
    "        {'params': model.vision_proj.parameters()},\n",
    "        {'params': model.cross_attn.parameters()},\n",
    "    ]\n",
    "\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.Adam(trainable_params, lr=1e-4)\n",
    "\n",
    "    # # 定义任务相关的损失函数(对比学习损失)\n",
    "    # criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "    # \n",
    "    \n",
    "    # 模拟输入\n",
    "    text_inputs = torch.randint(0, 49408, (2, 77))  # 文本输入\n",
    "    image_inputs = torch.randn(2, 3, 224, 224)     # 图像输入\n",
    "    \n",
    "    # 前向传播\n",
    "    text_to_vision, vision_to_text = model(input_ids=text_inputs, pixel_values=image_inputs)\n",
    "    # print(\"Output shape:\", output.shape)  # 预期输出形状 [2, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_to_vision.shape)\n",
    "# 取[CLS] token作为最终表示\n",
    "cls_output = output[:, 0, :]  # [batch, common_dim]\n",
    "print(cls_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n1 = np.random.randn(2,3)\n",
    "n2 = np.random.randn(6,3)\n",
    "n3 = n1 @ n2.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n1.shape)\n",
    "print(n2.shape)\n",
    "print(n3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "labels = torch.arange(4)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPPreTrainedModel, CLIPModel, CLIPProcessor, CLIPTokenizer, CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n",
    "from PIL import Image\n",
    "from typing import Optional, Tuple, Union, Any\n",
    "import requests\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        A single decoder block with cross-attention and feed forward network.\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, hidden_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "class CLIPWithProjectedCrossAttention(CLIPPreTrainedModel):\n",
    "    def __init__(self, config: CLIPConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # 加载CLIP模型并冻结参数\n",
    "        self.clip = CLIPModel(config)\n",
    "        for param in self.clip.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # 获取编码器的维度信息\n",
    "        text_hidden_size = config.text_config.hidden_size\n",
    "        vision_hidden_size = config.vision_config.hidden_size\n",
    "        common_dim = 768    # 统一投影维度\n",
    "\n",
    "        # # 维度对齐投影层（使用线性层）\n",
    "        # self.text_proj = nn.Linear(text_hidden_size, common_dim)\n",
    "        # self.vision_proj = nn.Linear(vision_hidden_size, common_dim)\n",
    "\n",
    "        # 增强的投影层（使用MLP）\n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(text_hidden_size, common_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(common_dim, common_dim)\n",
    "        )\n",
    "        self.vision_proj = nn.Sequential(\n",
    "            nn.Linear(vision_hidden_size, common_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(common_dim, common_dim)\n",
    "        )\n",
    "\n",
    "        # 交叉注意力层\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=common_dim,\n",
    "            num_heads=12, # 根据需要调整\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # # 初始化投影层参数\n",
    "        # nn.init.xavier_uniform_(self.text_proj.weight)\n",
    "        # nn.init.xavier_uniform_(self.vision_proj.weight)\n",
    "        # self.post_init()\n",
    "\n",
    "        # 参数初始化\n",
    "        nn.init.xavier_uniform_(self.text_proj[0].weight)\n",
    "        nn.init.xavier_uniform_(self.text_proj[2].weight)\n",
    "        nn.init.xavier_uniform_(self.vision_proj[0].weight)\n",
    "        nn.init.xavier_uniform_(self.vision_proj[2].weight)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, torch.Tensor]:\n",
    "        \n",
    "        # 获取编码器原始输出\n",
    "        text_outputs = self.clip.text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "        vision_outputs = self.clip.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "        # 获取最后一层隐藏状态（未投影）\n",
    "        text_features = text_outputs.last_hidden_state\n",
    "        vision_features = vision_outputs.last_hidden_state\n",
    "\n",
    "        # 维度对齐投影\n",
    "        projected_text = self.text_proj(text_features)          # (batch, seq_len. common_dim)\n",
    "        projected_vision = self.vision_proj(vision_features)    # (batch, seq_len, common_dim)\n",
    "        print(f\"projected_text.shape: {projected_text.shape}, projected_vision.shape: {projected_vision.shape}\")\n",
    "\n",
    "        # 归一化\n",
    "        projected_text = projected_text / projected_text.norm(dim=-1, keepdim=True)\n",
    "        projected_vision = projected_vision / projected_vision.norm(dim=-1, keepdim=True)\n",
    "        print(f\"projected_text.shape: {projected_text.shape}, projected_vision.shape: {projected_vision.shape}\")\n",
    "\n",
    "        # 交叉注意力计算（文本作为Query，视觉作为Key/Value）\n",
    "        text_to_vision, _ = self.cross_attn(\n",
    "            query=projected_text,\n",
    "            key=projected_vision,\n",
    "            value=projected_vision,\n",
    "            key_padding_mask=(attention_mask == 0) if attention_mask is not None else None\n",
    "        )\n",
    "        vision_to_text, _ = self.cross_attn(\n",
    "            query=projected_vision,\n",
    "            key=projected_text,\n",
    "            value=projected_text,\n",
    "            key_padding_mask=(attention_mask == 0) if attention_mask is not None else None\n",
    "        )\n",
    "\n",
    "        return text_to_vision, vision_to_text\n",
    "\n",
    "# 对比学习损失函数\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cross_entroy = nn.CrossEntropyLoss\n",
    "    \n",
    "    def forward(self, text_emb, vision_emb):\n",
    "        # 计算相似度矩阵\n",
    "        logits_per_text = text_emb @ vision_emb.t() / self.temperature\n",
    "        logits_per_vision = vision_emb @ text_emb.t() / self.temperature\n",
    "        labels = torch.arange(logits_per_text.size(0), device=text_emb.device)\n",
    "        loss_t = self.cross_entroy(logits_per_text, labels)\n",
    "        loss_v = self.cross_entroy(logits_per_vision, labels)\n",
    "        return (loss_t + loss_v) / 2\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化配置（示例使用不同维度）\n",
    "    text_config = CLIPTextConfig(hidden_size=512)\n",
    "    vision_config = CLIPVisionConfig(hidden_size=768)\n",
    "\n",
    "    text_config_dict = text_config.to_dict()\n",
    "    vision_config_dict = vision_config.to_dict()\n",
    "    config = CLIPConfig(\n",
    "        text_config=text_config_dict, \n",
    "        vision_config=vision_config_dict\n",
    "    )\n",
    "    \n",
    "    model = CLIPWithProjectedCrossAttention(config)\n",
    "    \n",
    "\n",
    "    # ====================训练模型====================\n",
    "    # 打印可训练参数\n",
    "    print(f\"{'='*10}Trainable parameters:{'='*10}\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{'-'*10}{name}\")\n",
    "\n",
    "    # 仅选择需要训练的参数\n",
    "    trainable_params = [\n",
    "        {'params': model.text_proj.parameters()},\n",
    "        {'params': model.vision_proj.parameters()},\n",
    "        {'params': model.cross_attn.parameters()},\n",
    "    ]\n",
    "\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.Adam(trainable_params, lr=1e-4)\n",
    "\n",
    "    # # 定义任务相关的损失函数(对比学习损失)\n",
    "    # criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "    # \n",
    "    \n",
    "    # 模拟输入\n",
    "    text_inputs = torch.randint(0, 49408, (2, 77))  # 文本输入\n",
    "    image_inputs = torch.randn(2, 3, 224, 224)     # 图像输入\n",
    "    \n",
    "    # 前向传播\n",
    "    text_to_vision, vision_to_text = model(input_ids=text_inputs, pixel_values=image_inputs)\n",
    "    # print(\"Output shape:\", output.shape)  # 预期输出形状 [2, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wxl_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
